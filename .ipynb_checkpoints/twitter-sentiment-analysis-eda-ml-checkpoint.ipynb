{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3428111,"sourceType":"datasetVersion","datasetId":2066095}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" <center><img src='https://miro.medium.com/v2/resize:fit:1280/1*TxNDsw1op7sxikKd6Sk08w.gif' \n     height=100px width=400px /></center> ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"toc\"></a>\n\n<p><center style=\"color:#ed5611; font-family: 'Dancing Script', cursive; font-size:25px;\">Thanks for visiting my notebook </center></p>\n\n<div style=\"background-color:#e4ebf7; padding: 10px;\">\n<div class=\"alert alert-block alert-secondary\" style=\"font-size:18px; font-family:verdana;\">  Feel free to customize or fork the notebook to suit your needs. If you find it helpful, please consider giving it an upvote ⬆️ — it helps others discover the notebook too! Your support motivates me to create more content like this</div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"0\"></a>\n\n\n <center><img src='https://i.imgur.com/MSRc2v3.jpg' \n     height=150px width=1150px /></center> \n     \n<div style=\"border-radius:10px; padding: 12px; background-color: #f2f6fc; font-size:100%; text-align:left\">\n\n<h2 align=\"left\"><font color=#00a4f9>Introduction</font></h2>\n\n<div class=\"alert alert-block alert-secondary\" style=\"font-size:20px; font-family:verdana;\">   \n  \nTwitter serves as an online social media platform where individuals express their thoughts through tweets. The issue of misuse, particularly for hateful content, has come to light. Twitter aims to address this concern by seeking assistance in developing a robust NLP-based classifier model. The objective is to differentiate negative tweets and prevent their dissemination.<br>\n\n    \nThe dataset comprises tweet text and corresponding sentiment labels. The training set includes a specific word or phrase (selected_text) extracted from the tweet, representing the provided sentiment.<br>\n\nThe goal is to predict the word or phrase within the tweet that best embodies the provided sentiment, encompassing all characters within that span, including commas and spaces.<br>\n    \n<h4 align=\"left\"><font color=#00a4f9>Columns</font></h4>\n- textID - unique ID for each piece of text<br>\n- text - the text of the tweet<br>\n- sentiment - the general sentiment of the tweet <br>\n\n<h4 align=\"left\"><font color=#00a4f9>Objective</font></h4>\n\n- Understand the Dataset & cleanup (if required)<br>\n- Build classification models to predict the twitter sentiments<br>\n- Compare the evaluation metrics of vaious classification algorithms<br>\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"toc\"></a>\n\n<a href=\"#toc\" style=\"background-color: #00a4f9; color: #ffffff; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a>\n<h2 align=\"left\"><font color=#00a4f9>Importing the Libraries</font></h2>","metadata":{}},{"cell_type":"code","source":"!pip install seaborn","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-01-04T10:12:23.438675Z","iopub.execute_input":"2024-01-04T10:12:23.439074Z","iopub.status.idle":"2024-01-04T10:12:36.697711Z","shell.execute_reply.started":"2024-01-04T10:12:23.439044Z","shell.execute_reply":"2024-01-04T10:12:36.696474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install nltk","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-01-04T10:12:36.699698Z","iopub.execute_input":"2024-01-04T10:12:36.700086Z","iopub.status.idle":"2024-01-04T10:12:47.649267Z","shell.execute_reply.started":"2024-01-04T10:12:36.700046Z","shell.execute_reply":"2024-01-04T10:12:47.647925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nimport string\nfrom tqdm import tqdm\nfrom multiprocessing import Pool\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, \\\nroc_auc_score, roc_curve, precision_score, recall_score\n\nimport warnings \nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-04T10:16:22.453697Z","iopub.execute_input":"2024-01-04T10:16:22.45421Z","iopub.status.idle":"2024-01-04T10:16:22.465562Z","shell.execute_reply.started":"2024-01-04T10:16:22.454155Z","shell.execute_reply":"2024-01-04T10:16:22.464418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the dataset \ndf = pd.read_csv('/kaggle/input/twitter-tweets-sentiment-dataset/Tweets.csv')\n#Let's check the samples of data\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:16:22.883621Z","iopub.execute_input":"2024-01-04T10:16:22.884293Z","iopub.status.idle":"2024-01-04T10:16:22.971988Z","shell.execute_reply.started":"2024-01-04T10:16:22.884259Z","shell.execute_reply":"2024-01-04T10:16:22.970638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"toc\"></a>\n\n<a href=\"#toc\" style=\"background-color: #009dff; color: #ffffff; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a>\n<h2 align=\"left\"><font color=#00a4f9>Exploratory Data Analysis</font></h2>","metadata":{}},{"cell_type":"code","source":"#Let's drop selected text & text id column\ndf.drop(['selected_text', 'textID'], axis=1, inplace=True)\ntarget = 'sentiment'\ndf.reset_index(drop=True, inplace=True) #Resetting the index\noriginal_df = df.copy(deep=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:16:23.903237Z","iopub.execute_input":"2024-01-04T10:16:23.903634Z","iopub.status.idle":"2024-01-04T10:16:23.922051Z","shell.execute_reply.started":"2024-01-04T10:16:23.903602Z","shell.execute_reply":"2024-01-04T10:16:23.920783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dimentions of the dataset & information about dataset\nprint('Dimentions of dataset:', df.shape)\n#Checking the dtypes of all the columns\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:16:24.287654Z","iopub.execute_input":"2024-01-04T10:16:24.288829Z","iopub.status.idle":"2024-01-04T10:16:24.312804Z","shell.execute_reply.started":"2024-01-04T10:16:24.288791Z","shell.execute_reply":"2024-01-04T10:16:24.31145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Descriptive summary of dataset\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:16:24.733128Z","iopub.execute_input":"2024-01-04T10:16:24.733617Z","iopub.status.idle":"2024-01-04T10:16:24.781115Z","shell.execute_reply.started":"2024-01-04T10:16:24.733582Z","shell.execute_reply":"2024-01-04T10:16:24.779742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 12px; background-color: #f2f6fc; font-size:100%; text-align:left\">\n\n<p style=\"font-size:18px; font-family:verdana; line-height: 1.7em\">- The datset contains 27481 rows and 2 columns.<br>\n- The text column includes 27480 non-null entries,'sentiment' column contains 27,481 non-null entries.<br>\n- Both columns are of type 'object.'<br>\n- The most frequent sentiment is 'neutral,' which appears 11,118 times.<br>\n- Three unique sentiment categories: 'positive,' 'negative,' and 'neutral.'</p>\n\n","metadata":{}},{"cell_type":"code","source":"#Let's check Null values\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:16:25.688529Z","iopub.execute_input":"2024-01-04T10:16:25.688928Z","iopub.status.idle":"2024-01-04T10:16:25.711071Z","shell.execute_reply.started":"2024-01-04T10:16:25.688897Z","shell.execute_reply":"2024-01-04T10:16:25.710119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" <p style=\"font-size:20px; font-family:verdana; line-height: 1.7em\"> The dataset has one null row, we can drop it</p>","metadata":{}},{"cell_type":"code","source":"#Dropping the null values\ndf.dropna(inplace=True)\noriginal_df = df.copy()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:16:26.528327Z","iopub.execute_input":"2024-01-04T10:16:26.528791Z","iopub.status.idle":"2024-01-04T10:16:26.550835Z","shell.execute_reply.started":"2024-01-04T10:16:26.528755Z","shell.execute_reply":"2024-01-04T10:16:26.549147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's check Duplicates\ndf.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:16:27.603965Z","iopub.execute_input":"2024-01-04T10:16:27.604382Z","iopub.status.idle":"2024-01-04T10:16:27.626286Z","shell.execute_reply.started":"2024-01-04T10:16:27.604352Z","shell.execute_reply":"2024-01-04T10:16:27.62491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em\"> No duplicate values found</p>","metadata":{}},{"cell_type":"code","source":"# Let's get a word count \ndf['word_count'] = df['text'].apply(lambda x: len(str(x).split(\" \")))\ndf[['text','word_count']].head()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:16:28.218679Z","iopub.execute_input":"2024-01-04T10:16:28.219082Z","iopub.status.idle":"2024-01-04T10:16:28.269575Z","shell.execute_reply.started":"2024-01-04T10:16:28.21905Z","shell.execute_reply":"2024-01-04T10:16:28.268421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Number of Characters- including spaces\ndf['char_count'] = df['text'].str.len() # this also includes spaces\ndf[['text','char_count']].head()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:16:28.91299Z","iopub.execute_input":"2024-01-04T10:16:28.913856Z","iopub.status.idle":"2024-01-04T10:16:28.938669Z","shell.execute_reply.started":"2024-01-04T10:16:28.913814Z","shell.execute_reply":"2024-01-04T10:16:28.937482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Average Word Length:\ndef avg_word(sentence):\n    words = sentence.split()\n    return (sum(len(word) for word in words)/len(words))\ndf['avg_word'] = df['text'].apply(lambda x: avg_word(x))\ndf[['text','avg_word']].head()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:16:29.953048Z","iopub.execute_input":"2024-01-04T10:16:29.953444Z","iopub.status.idle":"2024-01-04T10:16:30.049272Z","shell.execute_reply.started":"2024-01-04T10:16:29.953414Z","shell.execute_reply":"2024-01-04T10:16:30.04839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Number of stop Words:\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n\ndf['stopwords'] = df['text'].apply(lambda x: len([x for x in x.split() if x in stop]))\ndf[['text','stopwords']].head()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:16:32.143688Z","iopub.execute_input":"2024-01-04T10:16:32.14415Z","iopub.status.idle":"2024-01-04T10:16:32.97357Z","shell.execute_reply.started":"2024-01-04T10:16:32.144108Z","shell.execute_reply":"2024-01-04T10:16:32.972224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Number of special character:\ndf['hastags'] = df['text'].apply(lambda x: len([x for x in x.split() if x.startswith('@')]))\ndf[['text','hastags']].head()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:16:33.238015Z","iopub.execute_input":"2024-01-04T10:16:33.238681Z","iopub.status.idle":"2024-01-04T10:16:33.349442Z","shell.execute_reply.started":"2024-01-04T10:16:33.238645Z","shell.execute_reply":"2024-01-04T10:16:33.348114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Number of numerics:\ndf['numerics'] = df['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\ndf[['text','numerics']].head()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:16:34.868857Z","iopub.execute_input":"2024-01-04T10:16:34.869307Z","iopub.status.idle":"2024-01-04T10:16:34.949724Z","shell.execute_reply.started":"2024-01-04T10:16:34.869274Z","shell.execute_reply":"2024-01-04T10:16:34.948432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"toc\"></a>\n\n<a href=\"#toc\" style=\"background-color: #009dff; color: #ffffff; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a>\n<h2 align=\"left\"><font color=#00a4f9>Wordcloud</font></h2>\n\n<p style=\"font-size:18px; font-family:verdana; line-height: 1.7em\"> - Let's visualize the words </p> ","metadata":{}},{"cell_type":"code","source":"pip install wordcloud","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-01-04T10:12:51.522298Z","iopub.execute_input":"2024-01-04T10:12:51.523098Z","iopub.status.idle":"2024-01-04T10:13:02.427602Z","shell.execute_reply.started":"2024-01-04T10:12:51.523064Z","shell.execute_reply":"2024-01-04T10:13:02.426054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nnegative_df = df[df['sentiment'] == 'negative']\npositive_df = df[df['sentiment'] == 'positive']\nneutral_df = df[df['sentiment'] == 'neutral']\n\n# Define a function to generate and display a WordCloud\ndef generate_wordcloud(data, title):\n    words = ' '.join(data['text'])\n    cleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT' ])\n    wordcloud = WordCloud(stopwords=STOPWORDS,background_color='black', \n                          width=3000, height=800).generate(cleaned_word)\n    plt.figure(figsize=(15, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.title(title)\n    plt.axis('off')\n    plt.show()\n# Generate and display WordClouds for each sentiment category\ngenerate_wordcloud(negative_df, 'Negative Sentiment WordCloud')\ngenerate_wordcloud(positive_df, 'Positive Sentiment WordCloud')\ngenerate_wordcloud(neutral_df, 'Neutral Sentiment WordCloud')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:17:02.493579Z","iopub.execute_input":"2024-01-04T10:17:02.493978Z","iopub.status.idle":"2024-01-04T10:17:19.598515Z","shell.execute_reply.started":"2024-01-04T10:17:02.493948Z","shell.execute_reply":"2024-01-04T10:17:19.597625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em\">- In the above word cloud there are certain words probably will not make too much sense to study the reason for negative sentiment and subsequently based on the frequency of these words we will decide if the same needs to be deleted </p> \n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"toc\"></a>\n\n<a href=\"#toc\" style=\"background-color: #009dff; color: #ffffff; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a>\n<h2 align=\"left\"><font color=#00a4f9>Basic Pre-Processing</font></h2>","metadata":{}},{"cell_type":"code","source":"# Convert text to lowercase\ndf['text'] = df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n\n# Removal of punctuations\ndf['text'].str.replace('[^\\w\\s]','')\n\n#Removal of StopWords\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\ndf['text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ndf['text'].head()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:17:59.963887Z","iopub.execute_input":"2024-01-04T10:17:59.964396Z","iopub.status.idle":"2024-01-04T10:18:00.922432Z","shell.execute_reply.started":"2024-01-04T10:17:59.964357Z","shell.execute_reply":"2024-01-04T10:18:00.920928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"toc\"></a>\n\n<a href=\"#toc\" style=\"background-color: #009dff; color: #ffffff; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a>\n<h2 align=\"left\"><font color=#00a4f9>Common Words Removal</font></h2>\n\n<p style=\"font-size:18px; font-family:verdana; line-height: 1.7em\">- Let's create a list of 10 frequently occurring words and then decide if we need to remove it or retain it</p> ","metadata":{}},{"cell_type":"code","source":"freq = pd.Series(' '.join(df['text']).split()).value_counts()[:30]\nfreq","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:18:01.083292Z","iopub.execute_input":"2024-01-04T10:18:01.083895Z","iopub.status.idle":"2024-01-04T10:18:01.22271Z","shell.execute_reply.started":"2024-01-04T10:18:01.083855Z","shell.execute_reply":"2024-01-04T10:18:01.221264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em\">Let's remove \"I'm\", '-', '****', '& '<br>\n    There can be other words too which can be removed, but let's conbtinue with above only </p>  ","metadata":{}},{"cell_type":"code","source":"freq =[\"I'm\", \"-\", \"****\", \"&\"]\ndf['text']= df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ndf['text'].head()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:18:01.743906Z","iopub.execute_input":"2024-01-04T10:18:01.744376Z","iopub.status.idle":"2024-01-04T10:18:01.835778Z","shell.execute_reply.started":"2024-01-04T10:18:01.744341Z","shell.execute_reply":"2024-01-04T10:18:01.834283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Rare Words Removal\n#This is done as association of these less occurring words with the existing words could be a noise\nfreq = pd.Series(' '.join(df['text']).split()).value_counts()[-10:]\nfreq","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:18:02.344325Z","iopub.execute_input":"2024-01-04T10:18:02.34477Z","iopub.status.idle":"2024-01-04T10:18:02.480175Z","shell.execute_reply.started":"2024-01-04T10:18:02.344739Z","shell.execute_reply":"2024-01-04T10:18:02.478844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Stemming -refers to the removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach¶\nfrom nltk.stem import PorterStemmer\nst = PorterStemmer()\ndf['text'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:18:04.328018Z","iopub.execute_input":"2024-01-04T10:18:04.328445Z","iopub.status.idle":"2024-01-04T10:18:04.339786Z","shell.execute_reply.started":"2024-01-04T10:18:04.328414Z","shell.execute_reply":"2024-01-04T10:18:04.338718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[target].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:31:01.673053Z","iopub.execute_input":"2024-01-04T10:31:01.67344Z","iopub.status.idle":"2024-01-04T10:31:01.684952Z","shell.execute_reply.started":"2024-01-04T10:31:01.673412Z","shell.execute_reply":"2024-01-04T10:31:01.683278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's look at the overall distribution of positive, negative and neutral sentiments\nprint('\\033[1mSentiment Variable Distribution'.center(70))\nplt.figure(figsize=(12, 6))\ncolors = ['#ebde96', '#5ebd89', '#ff8680']\nplt.pie(df[target].value_counts(), labels=['Neutral', 'Positive', 'Negative'], counterclock=False, shadow=True,\n        explode=[0, 0, 0.15], autopct='%1.2f%%', radius=1, startangle=1, colors=colors)\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-04T10:34:19.468388Z","iopub.execute_input":"2024-01-04T10:34:19.46885Z","iopub.status.idle":"2024-01-04T10:34:19.631546Z","shell.execute_reply.started":"2024-01-04T10:34:19.468816Z","shell.execute_reply":"2024-01-04T10:34:19.630262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 align=\"left\"><font color=#00a4f9>Let's visualize wordcloud post cleaning</font></h3>","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nnegative_df = df[df['sentiment'] == 'negative']\npositive_df = df[df['sentiment'] == 'positive']\nneutral_df = df[df['sentiment'] == 'neutral']\n\n# Define a function to generate and display a WordCloud\ndef generate_wordcloud(data, title):\n    words = ' '.join(data['text'])\n    cleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT' ])\n    wordcloud = WordCloud(stopwords=STOPWORDS,background_color='black', \n                          width=3000, height=800).generate(cleaned_word)\n    plt.figure(figsize=(15, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.title(title)\n    plt.axis('off')\n    plt.show()\n# Generate and display WordClouds for each sentiment category\ngenerate_wordcloud(negative_df, 'Negative Sentiment WordCloud')\ngenerate_wordcloud(positive_df, 'Positive Sentiment WordCloud')\ngenerate_wordcloud(neutral_df, 'Neutral Sentiment WordCloud')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:28:28.83841Z","iopub.execute_input":"2024-01-04T10:28:28.838837Z","iopub.status.idle":"2024-01-04T10:28:46.354994Z","shell.execute_reply.started":"2024-01-04T10:28:28.838804Z","shell.execute_reply":"2024-01-04T10:28:46.353914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"toc\"></a>\n\n<a href=\"#toc\" style=\"background-color: #009dff; color: #ffffff; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a>\n\n<h2 align=\"left\"><font color=#00a4f9>Predictive Modeling</font></h2>","metadata":{}},{"cell_type":"code","source":"X = df['text']\ny = df['sentiment']","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:13:38.25078Z","iopub.execute_input":"2024-01-04T10:13:38.251111Z","iopub.status.idle":"2024-01-04T10:13:38.258727Z","shell.execute_reply.started":"2024-01-04T10:13:38.251082Z","shell.execute_reply":"2024-01-04T10:13:38.256979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer ()\nX = vectorizer.fit_transform(X).toarray()\nvectorizer","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:13:38.260072Z","iopub.execute_input":"2024-01-04T10:13:38.260447Z","iopub.status.idle":"2024-01-04T10:13:39.970766Z","shell.execute_reply.started":"2024-01-04T10:13:38.260409Z","shell.execute_reply":"2024-01-04T10:13:39.969384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:13:39.972257Z","iopub.execute_input":"2024-01-04T10:13:39.9727Z","iopub.status.idle":"2024-01-04T10:13:44.243564Z","shell.execute_reply.started":"2024-01-04T10:13:39.972672Z","shell.execute_reply":"2024-01-04T10:13:44.242075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let us create first create a table to store the results of various models \nresults_df = pd.DataFrame(np.zeros((2,5)), columns=['Accuracy', 'Precision','Recall','F1-score','AUC-ROC score'])\nresults_df.index=['Logistic Regression (LR)','Naïve Bayes Classifier (NB)'] #'Decision Tree Classifier (DT)', 'Linear Discriminant Analysis(LDA)','Random Forest Classifier (RF)', 'Support Vector Classifier (SVC)'\nresults_df","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:13:44.244948Z","iopub.execute_input":"2024-01-04T10:13:44.245279Z","iopub.status.idle":"2024-01-04T10:13:44.260432Z","shell.execute_reply.started":"2024-01-04T10:13:44.245252Z","shell.execute_reply":"2024-01-04T10:13:44.25941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install scikit-plot","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-01-04T10:13:44.262057Z","iopub.execute_input":"2024-01-04T10:13:44.262496Z","iopub.status.idle":"2024-01-04T10:13:55.25785Z","shell.execute_reply.started":"2024-01-04T10:13:44.262464Z","shell.execute_reply":"2024-01-04T10:13:55.256606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let us define functions to summarise the Prediction's scores .\nfrom scikitplot.metrics import plot_roc_curve as auc_roc\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, \\\nroc_auc_score, roc_curve, precision_score, recall_score\n\n#Classification Summary Function\ndef Classification_Summary(pred,pred_prob,i):\n    results_df.iloc[i]['Accuracy']=round(accuracy_score(y_test, pred),3)*100   \n    results_df.iloc[i]['Precision']=round(precision_score(y_test, pred, average='weighted'),3)*100 #, average='weighted'\n    results_df.iloc[i]['Recall']=round(recall_score(y_test, pred, average='weighted'),3)*100 #, average='weighted'\n    results_df.iloc[i]['F1-score']=round(f1_score(y_test, pred, average='weighted'),3)*100 #, average='weighted'\n    results_df.iloc[i]['AUC/ROC score']=round(roc_auc_score(y_test, pred_prob, multi_class='ovr'),3)*100 #, multi_class='ovr'\n    print('{}{}\\033[1m Evaluating {} \\033[0m{}{}\\n'.format('<'*3,'-'*35,results_df.index[i], '-'*35,'>'*3))\n    print('Accuracy = {}%'.format(round(accuracy_score(y_test, pred),3)*100))\n    print('F1 Score = {}%'.format(round(f1_score(y_test, pred, average='weighted'),3)*100)) #, average='weighted'\n    print('\\n \\033[1mConfusiton Matrix:\\033[0m\\n',confusion_matrix(y_test, pred))\n    print('\\n\\033[1mClassification Report:\\033[0m\\n',classification_report(y_test, pred))\n    \n    auc_roc(y_test, pred_prob, curves=['each_class'])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:13:55.259542Z","iopub.execute_input":"2024-01-04T10:13:55.25993Z","iopub.status.idle":"2024-01-04T10:13:55.303565Z","shell.execute_reply.started":"2024-01-04T10:13:55.259894Z","shell.execute_reply":"2024-01-04T10:13:55.301677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualising Function\ndef AUC_ROC_plot(y_test, pred):    \n    ref = [0 for _ in range(len(y_test))]\n    ref_auc = roc_auc_score(y_test, ref)\n    lr_auc = roc_auc_score(y_test, pred)\n\n    ns_fpr, ns_tpr, _ = roc_curve(y_test, ref)\n    lr_fpr, lr_tpr, _ = roc_curve(y_test, pred)\n\n    plt.plot(ns_fpr, ns_tpr, linestyle='--')\n    plt.plot(lr_fpr, lr_tpr, marker='.', label='AUC = {}'.format(round(roc_auc_score(y_test, pred)*100,2))) \n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:13:55.30497Z","iopub.execute_input":"2024-01-04T10:13:55.30623Z","iopub.status.idle":"2024-01-04T10:13:55.31516Z","shell.execute_reply.started":"2024-01-04T10:13:55.306162Z","shell.execute_reply":"2024-01-04T10:13:55.314272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#00a4f9>Logistics Regression Model</font></h2>","metadata":{}},{"cell_type":"code","source":"# Building Logistic Regression Classifier\nfrom sklearn.linear_model import LogisticRegression\n\nLR_model = LogisticRegression()\nLR = LR_model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:13:55.316665Z","iopub.execute_input":"2024-01-04T10:13:55.317761Z","iopub.status.idle":"2024-01-04T10:16:13.080594Z","shell.execute_reply.started":"2024-01-04T10:13:55.31772Z","shell.execute_reply":"2024-01-04T10:16:13.077552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = LR.predict(X_test)\npred_prob = LR.predict_proba(X_test)\nClassification_Summary(pred,pred_prob,0)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:16:13.082112Z","iopub.status.idle":"2024-01-04T10:16:13.082733Z","shell.execute_reply.started":"2024-01-04T10:16:13.082485Z","shell.execute_reply":"2024-01-04T10:16:13.082516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"toc\"></a>\n\n<a href=\"#toc\" style=\"background-color: #009dff; color: #ffffff; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a>\n<h2 align=\"left\"><font color=#00a4f9>Naive Bayes Classfier</font></h2>","metadata":{}},{"cell_type":"code","source":"#Naive Bayes Classfier\nNB_model = BernoulliNB()\nNB = NB_model.fit(X_train, y_train)\npred = NB.predict(X_test)\npred_prob = NB.predict_proba(X_test)\nClassification_Summary(pred,pred_prob,1)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:16:13.084665Z","iopub.status.idle":"2024-01-04T10:16:13.085076Z","shell.execute_reply.started":"2024-01-04T10:16:13.084896Z","shell.execute_reply":"2024-01-04T10:16:13.084913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"toc\"></a>\n\n<a href=\"#toc\" style=\"background-color: #009dff; color: #ffffff; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a>\n<h2 align=\"left\"><font color=#00a4f9>Decision Tree ModelClassfier</font></h2>","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"#Decision Tree classifier\n#DT_model = DecisionTreeClassifier()\n#DT = DT_model.fit(X_train, y_train)\n#pred = DT.predict(X_test)\n#pred_prob = DT.predict_proba(X_test)\n#Classification_Summary(pred,pred_prob,1)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-04T10:16:13.086657Z","iopub.status.idle":"2024-01-04T10:16:13.087053Z","shell.execute_reply.started":"2024-01-04T10:16:13.08687Z","shell.execute_reply":"2024-01-04T10:16:13.086887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"toc\"></a>\n\n<a href=\"#toc\" style=\"background-color: #00a4f9; color: #ffffff; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a>\n<h2 align=\"left\"><font color=#00a4f9>Discriminant Analysis</font></h2>","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"#Discriminant Analysis\n#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n#LDA_model = LinearDiscriminantAnalysis(solver='lsqr')  \n#LDA = LDA_model.fit(X_train, y_train)\n#pred = LDA.predict(X_test)\n#pred_prob = LDA.predict_proba(X_test)\n#Classification_Summary(pred,pred_prob,2)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-04T10:16:13.088137Z","iopub.status.idle":"2024-01-04T10:16:13.088552Z","shell.execute_reply.started":"2024-01-04T10:16:13.088378Z","shell.execute_reply":"2024-01-04T10:16:13.088397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"toc\"></a>\n\n<a href=\"#toc\" style=\"background-color: #00a4f9; color: #ffffff; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a>\n<h2 align=\"left\"><font color=#00a4f9>Random Forest Model</font></h2>","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"#Random Forest Model\n#RF_model = RandomForestClassifier()\n#RF = RF_model.fit(X_train, y_train)\n#pred = RF.predict(X_test)\n#pred_prob = RF.predict_proba(X_test)\n#Classification_Summary(pred,pred_prob,4)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-04T10:16:13.089683Z","iopub.status.idle":"2024-01-04T10:16:13.090044Z","shell.execute_reply.started":"2024-01-04T10:16:13.08988Z","shell.execute_reply":"2024-01-04T10:16:13.089896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"toc\"></a>\n\n<a href=\"#toc\" style=\"background-color: #00a4f9; color: #ffffff; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a>\n<h2 align=\"left\"><font color=#00a4f9>SVC</font></h2>","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"#SVC\n#SVC_model = SVC(probability=True)  \n#SVC_clf = SVC_model.fit(X_train, y_train) \n#pred = SVC_clf.predict(X_test)\n#pred_prob = SVC_clf.predict_proba(X_test)\n#Classification_Summary(pred, pred_prob, 5)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-04T10:16:13.091219Z","iopub.status.idle":"2024-01-04T10:16:13.091577Z","shell.execute_reply.started":"2024-01-04T10:16:13.091411Z","shell.execute_reply":"2024-01-04T10:16:13.091427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting Confusion-Matrix of all the predictive Models\n\nimport math\nlabels=['Positive','Negative']\ndef plot_cm(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.columns=labels\n    cm.index=labels\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    #fig, ax = plt.subplots()\n    sns.heatmap(cm, annot=annot, fmt='')# cmap= \"GnBu\"\n    \ndef conf_mat_plot(all_models):\n    plt.figure(figsize=[20,3*math.ceil(len([all_models])/4)])\n    \n    for i in range(len(all_models)):\n        if len(labels)<=6:\n            plt.subplot(1,6,i+1)\n        else:\n            plt.subplot(math.ceil(len(all_models)/2),2,i+1)\n        pred = all_models[i].predict(X_test)\n        #plot_cm(Test_Y, pred)\n        sns.heatmap(confusion_matrix(y_test, pred), annot=True, fmt='.0f') \n        plt.title(results_df.index[i])\n    plt.tight_layout()\n    plt.show()\n\nconf_mat_plot([LR, NB ]) #DT,RF,SVC_clf,LDA removed\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-04T10:16:13.092717Z","iopub.status.idle":"2024-01-04T10:16:13.093094Z","shell.execute_reply.started":"2024-01-04T10:16:13.092915Z","shell.execute_reply":"2024-01-04T10:16:13.092934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Comparing all the models Scores\ndisplay(\"Model Comparision\",results_df)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-04T10:16:13.09562Z","iopub.status.idle":"2024-01-04T10:16:13.09608Z","shell.execute_reply.started":"2024-01-04T10:16:13.095885Z","shell.execute_reply":"2024-01-04T10:16:13.095904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```Note: Not all models are run due to high computaional time. Codes are marked hidden ```\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 12px; background-color: #f2f6fc; font-size:100%; text-align:left\">\n\n<h2 align=\"left\"><font color=#00a4f9> Final Words </font></h2>\n<p style=\"font-size:18px; font-family:verdana; line-height: 1.7em\">Since the Sentiment is imbalanced, and finding actual sentiment of the person is important,the metric to watch out for in this case can be F1 score which is nothing but the harmonic mean between precision and recall.</p>\n\n<p style=\"font-size:18px; font-family:verdana; line-height: 1.7em\">Precision and Recall are complementary metrics that have an inverse relationship. If both are of interest to us then we use the F1 score to combine precision and recall into a single metric.</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:16px; font-family:verdana\">Reference :\n    \n For ML [ Predicting Twitter Sentiments - (Top ML Models)](https://www.kaggle.com/code/yasserh/predicting-twitter-sentiments-top-ml-models)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"toc\"></a>\n\n<a href=\"#toc\" style=\"background-color: #00a4f9; color: #ffffff; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a>\n\n---\n<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em; color:#00a4f9;\">\n    <em>Appreciate your time exploring my work. If you enjoyed it, kindly consider upvoting or feel free to drop comment / feedback to help enhance the notebook. Happy Learning!</em>\n</p>\n\n\n<center><img src='https://mir-s3-cdn-cf.behance.net/project_modules/1400/dea3c9102507757.5f3adfd3a2781.gif' \n     height=100px width=300px /></center>","metadata":{}}]}